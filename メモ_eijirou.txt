・最後の審判について先生に確認する。
    ・実装がチェックされるか。
    ・実装していないことによる成績上の不利はあるか。


・全体的な方針について
    ・時間が許す限り、色々なモデルを作って強いものを採用したい。


・機械学習（主にニューラルネットワーク）について
    ・NN自体はできそう。
        ・自作NNを用いてMNISTの(4,9)の2分類を行なったところ、92%の精度を出せた。
        ・(0,1)の2分類なら99%以上の精度が出る。
    ・どう学習させるかが難しい。
    ・ランダムな遷移を繰り返すものを作ることで、ある程度バリエーションがある棋譜を大量に生成できる。
        ・駒を打つ確率は少し下げたほうがよさそう。
        ・動かす駒を先に決める？
    ・詰みと詰みの直前に関しては教師あり学習を行うことができる。
    ・1手先にNNを適用することで、自己対戦による学習をある程度高速に行うことができる。
        ・初期盤面を変えることで、局面にバリエーションをもたせることができる。
    ・左右反転によってデータ拡張をすることができる。
    ・今のところ、NNの適用は10秒間に10^4回程度と見積もっている。


・NNの暫定的なモデル
    ・NNUE評価関数を参考にした。
    ・535 -Affine-> 32 -ReLU-> 32 -Affine-> 32 -ReLU-> 32 -Affine-> 1 -Sigmoid-> 1
    ・optimizer: Adam(lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-7)
    ・誤差逆伝播法において、勾配消失に対処するため、Sigmoid関数の微分を1とした。
    ・盤面の各マスを21次元のone-hot-encodingで表し、持ち駒を10次元で表すと、入力次元数が535になる。
    ・学習がうまくいかなかった場合、入力形式やNNのモデルを変えるとよさそう。
        ・マスにある駒ではなく、マスに利きがある駒を1にする入力形式も考えられる。
            ・入力の0と1のバランスがよくなって、汎用性が高くなるかもしれない。
        ・NNUEではHalfKPという特徴量を入力として与えている。


・モンテカルロ木探索の次にやりたいこと
    ・探索について
        ・評価関数を用いて、上位n個以外の指手を枝刈りする。
            ・n=4ぐらいだと思っているが、評価関数の質にもよる。
            ・可能な指手がn個未満のこともある。
        ・Mini-Max法を実装する。
        ・αカット、βカットを実装する。
        ・千日手をふまえて評価値を操作する。
    ・評価関数について
        ・ランダム遷移シミュレーションで大量の棋譜を生成する。
        ・詰みについて教師あり学習を行う。
        ・初期盤面を変えながら、自己対戦による学習を行う。


・ゲームシステム確認事項

    ・可能な指手を全て出力して、駒の動きの正当性
    ・成った駒の動き
    ・飛車や角の動き
    ・金や王が成れないこと
    ・歩が成れるときに必ず成ること
    ・銀、飛車、角が成っても成らなくてもよいこと
    ・敵陣から離れるときにも成れること

    ・二歩を打てないこと
    ・敵陣に歩を打てないこと
    ・打ち歩詰めができないこと
    ・王手放置ができないこと
    ・駒を動かして自分の王が王手状態にできないこと

    ・ステイルメイトが詰みとみなされること（ステイルメイトの場合は反則手を指す前に負けにするという理解で合ってる？）
    ・詰みのときにゲームが終了すること
    ・反則手を指したときにゲームが終了すること
    ・千日手のときに後手が勝つこと
    ・連続王手千日手のときに王手されている側が勝つこと

    ・打ち歩詰めと連続王手千日手が絡んだときは打ち歩詰めとみなすこと（多分そうなっていない。どうしよう。）

    ・打ち歩ステイルメイトは打てないようになっているはず


12/14に話したいこと
・進捗状況の確認
・実行およびバグについて
・最後の審判について
・ランダム遷移について
    ・さまざまな棋譜を大量に作成できる。
    ・棋譜を作っておけば、モデルの学習に使えるかも。
・モンテカルロ法について
    ・弱そうだが、もしかしたら強い。
    ・「半分の時間が経ったら、悪そうな候補を半分削除する」みたいな工夫をするといいかも。
    ・ループにはまらない工夫（例えば150手制限）が必要かも。
・探索について
    ・Mini-Max法（Nega-Max法）について
    ・αカット、βカットによる枝刈りについて
   　　・その他の手法について
    ・評価関数による枝刈りについて
    ・千日手について
・評価関数について
    ・駒得評価関数は不十分な気がする。
    ・KPP(King Piece Piece)などが知られているが、パラメータの数が多いらしい。
    ・Neural Networkもよく使われているが、入力が盤面ではなく特徴量（例えばKPP）であることも。
    ・個人的にはNNを作りたいが、まともな評価関数を作れる自信はない。
・次の担当について


/*
main関数の一部を書きました。
色々と雑に書いています。

先手番と後手番で4つに場合分けをしているのはわざとです。
*/

#define MAX_TURN 150
#define FIRST 1
#define SECOND 0


Action get_user_action(int turn){
    //scanf
    //Action action = string_to_action
    if (turn % 2 == 0)
        // 後手番のとき
        reverse_action(action);
    return action;
}

void display_action(Action action, int turn){
    if (turn % 2 == 0)
        // 後手番のとき
        reverse_action(action);
    printf("%s", action_to_string(action));
}


int main(void){

    // 実行時引数の処理
    int first_is_user = 1;
    int second_is_user = 0;



    Board board = create_board();
    print_board_for_debug(&board);

    //history = ; 盤面の履歴
    int winner = -1;

    for (int turn = 1; turn <= MAX_TURN; turn++){
        Action action;

        if (turn % 2){
            // 先手番
            if (first_is_user)
                action = get_user_action(turn);
            else{
                action = get_ai_action(&board, history, turn);
                display_action(action);
            }
        }
        else{
            // 後手番
            if (second_is_user)
                action = get_user_action(turn);
            else{
                action = get_ai_action(&board, history, turn);
                display_action(action, turn);
            }
        }

        winner = move_piece(&board, action, history, turn);
        print_board_for_debug(&board);

        if (winner != -1)
            break;
        
        reverse_board(&board);
    }

    // 出力
}
